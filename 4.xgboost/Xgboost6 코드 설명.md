
# Xgboost6 코드 설명



* `Xgboost5`에서 변경 한 부분만 설명하였습니다.



**XGBoost 모델을 CPU만으로 학습을 진행하니, 너무 오래걸렸다. **
**새로운 방법을 찾아 GPU를 사용하여 학습시켜보려한다.** 



XGBoost 모델을 GPU를 사용하여 학습하고 예측을 수행하면 다음과 같은 장점이 있다.

1. **빠른 학습과 예측**: GPU는 대규모 행렬 연산을 빠르게 처리할 수 있기 때문에, CPU에 비해 XGBoost 모델의 학습과 예측 속도를 크게 향상시킬 수 있다.
2. **대용량 데이터 처리**: 대용량 데이터를 처리할 때, GPU를 사용하면 처리 시간을 대폭 줄일 수 있다. 이는 XGBoost 모델의 학습과 예측 뿐 아니라, 대용량 데이터를 전처리하는 과정에서도 유용하다.
3. **모델 성능 개선**: GPU를 사용하면 모델 학습에 필요한 시간을 단축시켜 더 많은 하이퍼파라미터를 탐색할 수 있다. 이는 모델의 성능을 더욱 개선시킬 수 있는데, 더 좋은 성능을 얻기 위해서는 다양한 하이퍼파라미터를 시도해 봐야하기 때문이다.
4. **비용 절감**: GPU를 사용하여 모델 학습을 더욱 빠르게 수행할 수 있으므로, 모델 학습에 필요한 인프라 비용을 줄일 수 있다. 이는 대규모 데이터를 처리하는 기업이나 연구자들에게 큰 이점이 된다.



GPU 사용 가능 여부를 확인하기 위해 코드 맨 위에 torch 라이브러리를 가져온다.

```python
import torch
```



시스템에 GPU가 있는지 확인한다.

```python
# GPU 확인
if torch.cuda.is_available():
    print('GPU가 있습니다.')
else:
    print('GPU가 없습니다.')
```



XGBoost 모델 생성 시 GPU를 사용하도록 설정한다.

```python
xgb_model = xgb.XGBClassifier(tree_method='gpu_hist', gpu_id=0)
```

